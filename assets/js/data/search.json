[ { "title": "Probability Inequalities", "url": "/posts/probabiity-inequalities/", "categories": "deep-learning, computer-science", "tags": "ai, ml", "date": "2022-09-28 12:00:00 -0300", "snippet": "Scratches" }, { "title": "PAC Learning", "url": "/posts/pac-learning/", "categories": "deep-learning, computer-science", "tags": "ai, ml", "date": "2022-09-28 12:00:00 -0300", "snippet": "FROM: FOUNDATIONS OF ML Ch. 2“Machine learning is fundamentally about generalization”, p. 7.“The problem is typically formulated as that of selecting a function out of a hypothesis set, that is a subset of the family of all functions.”How should we define the complexity of a hypothesis set?“Several fundamental questions arise when designing and analyzing algorithms that learn from examples: What can be learned efficiently? What is inherently hard to learn? How many examples are needed to learn successfully? Is there a general model of learning?”, p. 9.Probably Approximately Correct (PAC)“The PAC framework helps define the class of learnable concepts in terms of the number of sample points needed to achieve an approximate solution, sample complexity, and the time and space complexity of the learning algorithm, which depends on the cost of the computational representation of the concepts.”The concept $c(x)$ is the “true” concept we want to learn.DEFINITION PAC-Learning: A concept class $\\mathcal{C}$ is said to be PAC-learnable if there exists an algorithm $\\mathcal{A}$ and a polynomial function $poly(·, ·, ·, ·)$ such that for any $\\epsilon &gt; 0$ and $\\delta &gt; 0$, for all distributions $\\mathcal{D}$ on $\\mathcal{X}$ and for any target concept $c \\in \\mathcal{C}$, the following holds for any sample size $m \\geq poly(1/\\epsilon, 1/\\delta, n, size(c))$:" }, { "title": "Model Capacity", "url": "/posts/model-capacity/", "categories": "deep-learning, computer-science", "tags": "ai, ml", "date": "2022-09-28 12:00:00 -0300", "snippet": "Scratcheshttps://dengking.github.io/machine-learning/Theory/Deep-learning/Guide/Model-capacity/Model-capacity/“The capacity of a deep learning neural network model controls the scope of the types of mapping functions that it is able to learn.”" }, { "title": "Can I Learn It?", "url": "/posts/is-learning-feasible/", "categories": "deep-learning, computer-science", "tags": "ai, ml", "date": "2022-09-28 12:00:00 -0300", "snippet": "Scratcheshttps://dengking.github.io/machine-learning/Theory/Deep-learning/Guide/Model-capacity/Model-capacity/“The capacity of a deep learning neural network model controls the scope of the types of mapping functions that it is able to learn.”Is Learning Feasible?What constitute a learning problem?ECE595ML Lecture 22-1 Is Learning Feasible?Quando treinamos um algoritmo (writing a paper) a diferenca entre o training e test set costuma ser na ordem de uma ou duas magnitudes (exemplos classicios do MNIST 60000/10000). Mas quando treinamos um algoritmo para funcionar no mundo real a diferenca entre o universo que ele foi treinado é muito maior: mesmo com um dataset de milhoes de imagens como IMAGENET, comparado a quantidade de imagens na internet, é infinitesimal.IN-SAMPLE/OUT-SAMPLE (dentro/fora do dataset) Hypothesis set: $\\mathcal{H} = { h_{1}, …, h_{m} }:$ Possible decision boundaries. Algorithm: Picks $h_{m}$ from $\\mathcal{H}$. Final hypothesis: g (The one we find)O conjunto $\\mathcal{H}$ vai variar de acordo com os métodos e os modelos usados para atacar o problema.Is learning feasible?Suppose we have a training set $\\mathcal{D}$, can we learn the target function $f$? “Learn” means I came up with a f with the given data. “Successful” means all IN-SAMPLES are correctly predicted AND all OUT-SAMPLES are also correctly predicted. If YES, then learning is feasible.In others words, can we generalize from the training set to the test set (or to the outworld unseen data)?Training vs TestingIn-Sample Error (Training Error) $x_{n}$ is a training sample if $h(x_{n}) = f(x_{n})$, then we say that the training sample is correctly classified.DEFINITION (In-Sample Error / Training Error):Consider a training set $\\mathcal{D} = { x_{1}, …, x_{n} }$, and a target function $f$. The in-sample error of a hypothesis function $h \\in \\mathcal{H}$ is the empirical average of ${ h(x_{n}) \\neq f(x_{n}) }$:$\\begin{equation}E_{in}(h) {\\stackrel{\\text{def}}{=}} \\frac {1}{N} \\sum [![ h(x_{n}) \\neq f(x_{n})]!]\\end{equation}$where $[![ \\cdot ]!] = 1 $ if the statement inside the brackets is true, and $= 0$ if the statement is false.Out-Sample Error (Testing Error) $x$ is a test sample drawn from $p(x)$, where $p(x)$ is the unkwown input distribution. if $h(x) = f(x)$, then we say that the training sample is correctly classified. since $x \\sim p(x)$, we need to compute the probability error, called the out-sample error.DEFINITION (Out-Sample Error / Testing Error):Consider an input space $\\mathcal{X}$ containing elements $x$ drawn from a distribution $p_{x}(x)$, and a target function $f$. The out-sample error of a hypothesis function $h \\in \\mathcal{H}$ is$\\begin{equation}E_{out}(h) {\\stackrel{\\text{def}}{=}} \\mathbb{P} [\\ h(x) \\neq f(x) ]\\ \\end{equation}$where $\\mathbb{P} [\\ \\cdot]\\ $ measures the probability of the statement based on the distribution $p_{x}(x)$.Because P is a Bernoulli distribution we can expand it as the expected value..The Role of $p(x)$If there is no correlation between the training and the test set learning is infeasible Learning is feasible if $x \\sim p(x)$. $p(x)$ says: Training and testing are related. If you draw training and testing with different bias, then you will sufferUsing probability inequalities (in this case, Hoeffding Inequality) we can relate trainig and testing.Interpreting the BoundMessage 1: You cand bound $E_{out}(h)$ using $E_{in}(h)$. $E_{in}(h)$: you know. $E_{out}(h)$: you don’t know, but you want to know. They are close if N is large.Message 2: The right hand side is independent of $h$ and $p(x)$. So it is a universal upper bound. Works for any $\\mathcal{A}$, any $\\mathcal{H}$, any $f$, and any $p(x)$.READING LIST High Dimensional Statistics ch. 2 Learning from data ch. 1 Foundations of ML ch. 2FROM: FOUNDATIONS OF ML Ch. 2“Machine learning is fundamentally about generalization”, p. 7.“The problem is typically formulated as that of selecting a function out of a hypothesis set, that is a subset of the family of all functions.”How should we define the complexity of a hypothesis set?“Several fundamental questions arise when designing and analyzing algorithms that learn from examples: What can be learned efficiently? What is inherently hard to learn? How many examples are needed to learn successfully? Is there a general model of learning?”, p. 9.Probably Approximately Correct (PAC)“The PAC framework helps define the class of learnable concepts in terms of the number of sample points needed to achieve an approximate solution, sample complexity, and the time and space complexity of the learning algorithm, which depends on the cost of the computational representation of the concepts.”The concept $c(x)$ is the “true” concept we want to learnRelação entre Input Space e Feature SpaceInput Space of a MNIST problem:Image size: 28 * 28Pixel depth: 256Input $size = 256^(28*28)$ = Absurdamente grandehttps://datascience.stackexchange.com/questions/60617/could-anyone-explain-these-terms-input-space-feature-space-sample-spacehttps://stats.stackexchange.com/questions/226369/what-is-an-input-space-and-why-does-the-fraction-of-the-input-space-covered-byhttps://stats.stackexchange.com/questions/46425/what-is-feature-space\"Generalizing correctly becomes exponentially harder as the dimensionality (number of features) of the examples grows, because a fixed-size training set covers a dwindling fraction of the input space. Even with a moderate dimension of 100 and a huge training set of a trillion examples, the latter covers only a fraction of about 10−18 of the input space. This is what makes machine learning both necessary and hard.\"Hypothesis Space also includes the output space. So for the MNIST case it would be:10^256^(28*28)" }, { "title": "Understanding Deep Learning Requires Rethinking Generalization", "url": "/posts/understanding-dl-generalization/", "categories": "deep-learning, computer-science", "tags": "ai, ml", "date": "2022-07-25 12:00:00 -0300", "snippet": "REFERENCESPapers to read: Generalization Error in Deep Learning Understanding deep learning requires rethinking generalization For self-supervised learning, Rationality implies generalization, provablyVideos and presentations https://www.youtube.com/watch?v=O42vde4tbG0&amp;t=233s https://www.youtube.com/watch?v=wi9mjnDfS7Y&amp;list=PLFE-LjWAAP9Q74cGaUF3yqUhqo2kOYY20&amp;index=2&amp;t=1316sCode https://paperswithcode.com/paper/understanding-deep-learning-requires https://github.com/pluskid/fitting-random-labelsBackground to know before reading Generalization Error Model Capacity Universal Approximation Theory VC Dimension Regularization Explicit Implicit " }, { "title": "First Test", "url": "/posts/hello-people/", "categories": "philosophy, computer-science", "tags": "ai, ml", "date": "2022-07-25 12:00:00 -0300", "snippet": "Heading 1Today, the most powerful artificial intelligence systems employ a type of machine learning called deep learning. Their algorithms learn by processing massive amounts of data through hidden layers of interconnected nodes, referred to as deep neural networks. As their name suggests, deep neural networks were inspired by the real neural networks in the brain, with the nodes modeled after real neurons — or, at least, after what neuroscientists knew about neurons back in the 1950s, when an influential neuron model called the perceptron was born. Since then, our understanding of the computational complexity of single neurons has dramatically expanded, so biological neurons are known to be more complex than artificial ones. But by how much?To find out, David Beniaguev, Idan Segev and Michael London, all at the Hebrew University of Jerusalem, trained an artificial deep neural network to mimic the computations of a simulated biological neuron. They showed that a deep neural network requires between five and eight layers of interconnected “neurons” to represent the complexity of one single biological neuron.But the study’s authors caution that it’s not a straightforward correspondence yet. “The relationship between how many layers you have in a neural network and the complexity of the network is not obvious,” said London. So we can’t really say how much more complexity is gained by moving from, say, four layers to five. Nor can we say that the need for 1,000 artificial neurons means that a biological neuron is exactly 1,000 times as complex. Ultimately, it’s possible that using exponentially more artificial neurons within each layer would eventually lead to a deep neural network with one single layer — but it would likely require much more data and time for the algorithm to learn.class Test(Enum): a: int def __init__(self): self.a = 1“We tried many, many architectures with many depths and many things, and mostly failed,” said London. The authors have shared their code to encourage other researchers to find a clever solution with fewer layers. But, given how difficult it was to find a deep neural network that could imitate the neuron with 99% accuracy, the authors are confident that their result does provide a meaningful comparison for further research. Lillicrap suggested it might offer a new way to relate image classification networks, which often require upward of 50 layers, to the brain. If each biological neuron is like a five-layer artificial neural network, then perhaps an image classification network with 50 layers is equivalent to 10 real neurons in a biological network.Heading 2: Some stuffThe authors also hope that their result will change the present state-of-the-art deep network architecture in AI. “We call for the replacement of the deep network technology to make it closer to how the brain works by replacing each simple unit in the deep network today with a unit that represents a neuron, which is already — on its own — deep,” said Segev. In this replacement scenario, AI researchers and engineers could plug in a five-layer deep network as a “mini network” to replace every artificial neuron.But some wonder whether this would really benefit AI. “I think that’s an open question, whether there’s an actual computational advantage,” said Anthony Zador, a neuroscientist at Cold Spring Harbor Laboratory. “This [work] lays the foundation for testing that.”" } ]
